# Survey
- [2025/04] **[A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment](https://arxiv.org/abs/2504.15585)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/02] **[On the Trustworthiness of Generative Foundation Models: Guideline, Assessment, and Perspective](https://arxiv.org/abs/2502.14296)**
- [2025/02] **[Safety at Scale: A Comprehensive Survey of Large Model Safety](https://arxiv.org/abs/2502.05206)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/11] **[Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based Agents](https://arxiv.org/abs/2411.09523)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2024/09] **[Harmful Fine-tuning Attacks and Defenses for Large Language Models: A Survey](https://arxiv.org/abs/2409.18169)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/git-disl/awesome_LLM-harmful-fine-tuning-papers) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/08] **[Attacks and Defenses for Generative Diffusion Models: A Comprehensive Survey](https://arxiv.org/abs/2408.03400)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/07] **[Operationalizing a Threat Model for Red-Teaming Large Language Models (LLMs)](https://arxiv.org/abs/2407.14937)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/07] **[AI Safety in Generative AI Large Language Models: A Survey](https://arxiv.org/abs/2407.18369)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/07] **[The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies](https://arxiv.org/abs/2407.19354)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2024/07] **[Operationalizing a Threat Model for Red-Teaming Large Language Models (LLMs)](https://arxiv.org/abs/2407.14937)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[Unique Security and Privacy Threats of Large Language Model: A Comprehensive Survey](https://arxiv.org/abs/2406.07973)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[Safeguarding Large Language Models: A Survey](https://arxiv.org/abs/2406.02622)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[Exploring Vulnerabilities and Protections in Large Language Models: A Survey](https://arxiv.org/abs/2406.00240)**
- [2024/03] **[Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression](https://arxiv.org/abs/2403.15447)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://decoding-comp-trust.github.io/) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices](https://arxiv.org/abs/2403.12503)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models](https://arxiv.org/abs/2403.04786)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[On Protecting the Data Privacy of Large Language Models (LLMs): A Survey](https://arxiv.org/abs/2403.05156)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Privacy](https://img.shields.io/badge/Privacy-87b800)
- [2024/02] **[Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey](https://arxiv.org/abs/2402.09283)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Safety](https://img.shields.io/badge/Safety-87b800)
- [2024/02] **[Safety of Multimodal Large Language Models on Images and Text ](https://arxiv.org//abs/2402.00357)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Safety](https://img.shields.io/badge/Safety-87b800)
- [2024/01] **[Security and Privacy Challenges of Large Language Models: A Survey](https://arxiv.org/abs/2402.00888)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/01] **[Black-Box Access is Insufficient for Rigorous AI Audits](https://arxiv.org/abs/2401.14446)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/01] **[Red Teaming Visual Language Models](https://arxiv.org/abs/2401.12915)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/01] **[R-Judge: Benchmarking Safety Risk Awareness for LLM Agents](https://arxiv.org/abs/2401.10019)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Safety](https://img.shields.io/badge/Safety-87b800) ![_Agent](https://img.shields.io/badge/_Agent-87b800)
- [2024/01] **[Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems](https://arxiv.org/abs/2401.05778)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/01] **[TrustLLM: Trustworthiness in Large Language Models](https://arxiv.org/abs/2401.05561)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/HowieHwong/TrustLLM) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICML'24](https://img.shields.io/badge/ICML'24-f1b800)
- [2023/12] **[Privacy Issues in Large Language Models: A Survey](https://arxiv.org/abs/2312.06717)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Privacy](https://img.shields.io/badge/Privacy-87b800)
- [2023/12] **[A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly](https://arxiv.org/abs/2312.02003)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/09] **[AgentBench: Evaluating LLMs as Agents](https://openreview.net/forum?id=zAdUB0aCTQ)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Identifying the Risks of LM Agents with an LM-Emulated Sandbox](https://openreview.net/forum?id=GEcwtMk1uA)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/07] **[A Comprehensive Overview of Large Language Models](https://arxiv.org/abs/2307.06435)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/06] **[DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models](https://arxiv.org/abs/2306.11698)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://decodingtrust.github.io/) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![NeurIPS'23](https://img.shields.io/badge/NeurIPS'23-f1b800) ![Best Paper](https://img.shields.io/badge/Best_paper-ff0000)
- [2023/04] **[Safety Assessment of Chinese Large Language Models](https://arxiv.org/abs/2304.10436)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Safety](https://img.shields.io/badge/Safety-87b800) ![_Chinese](https://img.shields.io/badge/_Chinese-87b800)
- [2023/03] **[A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2022/11] **[Holistic Evaluation of Language Models](https://arxiv.org/abs/2211.09110)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![TMLR'23](https://img.shields.io/badge/TMLR'23-f1b800)
- [2022/08] **[Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned](https://arxiv.org/abs/2209.07858)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2022/06] **[Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models](https://arxiv.org/abs/2206.04615)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
