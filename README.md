# Awesome AI Governance

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
[![Stars](https://img.shields.io/github/stars/YourRepo/Awesome-AI-Governance)](https://github.com/YourRepo/Awesome-AI-Governance/stargazers)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)

## Introduction

This repository serves as a comprehensive resource hub for **AI Governance**. The rapid advancement of AI has introduced complex technical vulnerabilities and societal risks, underscoring the pressing need for a holistic governance framework. This collection aims to bridge the gap between technical research, policy-making, and real-world application by curating not only academic papers but also key **policies and regulations**, impactful **news and case studies**, practical **technical tools**, and crucial **datasets**.

Our organizing principle is derived from the framework in our survey, "[Never Compromise to Vulnerabilities: A Comprehensive Survey on Al Governance](https://example.com/link_to_your_paper.pdf)," which categorizes AI Governance into three pillars:
* **Intrinsic Security**: Internal system reliability and robustness.
* **Derivative Security**: External, real-world harms from AI deployment.
* **Social Ethics**: Value alignment, accountability, and societal impact.

This repository aims to provide researchers, engineers, policymakers, and the public with a structured guide to navigating the multifaceted landscape of AI Governance.

- This repo is a living document and is actively being updated :seedling:.
- **Badges:**
    - **Model:**
        - ![LLM](https://img.shields.io/badge/LLM-589cf4)
        - ![VLM](https://img.shields.io/badge/VLM-c7688b)
        - ![AIGC](https://img.shields.io/badge/AIGC-a99cf4)
        - ![Agent](https://img.shields.io/badge/Agent-964B00)
    - **Tag:** ![Benchmark](https://img.shields.io/badge/Benchmark-87b800) ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800) ![Defense](https://img.shields.io/badge/Defense-87b800) ![Policy](https://img.shields.io/badge/Policy-87b800) ![Evaluation](https://img.shields.io/badge/Evaluation-87b800) ![Framework](https://img.shields.io/badge/Framework-87b800) ![Dataset](https://img.shields.io/badge/Dataset-87b800)...
    - **Venue:** ![TPAMI](https://img.shields.io/badge/TPAMI-f1b800) ![CVPR](https://img.shields.io/badge/CVPR-f1b800) ![ICCV](https://img.shields.io/badge/ICCV-f1b800) ![ACL](https://img.shields.io/badge/ACL-f1b800) ![NeurIPS](https://img.shields.io/badge/NeurIPS-f1b800) ![ICML](https://img.shields.io/badge/ICML-f1b800) ![USENIX](https://img.shields.io/badge/USENIX-f1b800)...

:sunflower: We welcome contributions! Please feel free to open a pull request or issue to add more resources. For academic papers, please use the format below. For other resources like policies or tools, the "Classification" column can be adapted accordingly (e.g., "EU AI Act", "Detection Tool").

| Title | Link | Code/Source | Venue/Issuer | Classification | Model | Tag |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| Paper Title | arxiv | github | Venue'YY | 1.1 Adversarial Vulnerability | LLM | Defense |

---
## News
- **[2025.08.08]** &#x1f680; Repository launched! This is a comprehensive hub for AI Governance, including papers, policies, tools, and datasets, with a detailed taxonomy based on our TPAMI 2025 survey.

## Collections

- [**Policies & Regulations**](collection/policies.md) (XX)
- [**News & Case Studies**](collection/news_cases.md) (XX)
- [**Technical Tools & Toolkits**](collection/toolkits.md) (XX)
- [Leaderboards](collection/leaderboards.md) (XX)
- [Surveys](collection/surveys.md) (XX)
- [Books](collection/books.md) (XX)
- **Papers** (XXXX)
    - **1. Intrinsic Security** (XXX)
        - [1.1 Adversarial Vulnerability](collection/paper/intrinsic_security/adversarial_vulnerability.md) (XX)
        - [1.2 Robustness](collection/paper/intrinsic_security/robustness.md) (XX)
        - [1.3 Hallucination](collection/paper/intrinsic_security/hallucination.md) (XX)
        - [1.4 Interpretability](collection/paper/intrinsic_security/interpretability.md) (XX)
    - **2. Derivative Security** (XXX)
        - [2.1 Privacy Risks](collections/papers/derivate_security/privacy.md) (XX)
        - [2.2 Bias & Discrimination](collections/papers/derivate_security/bias_discrimination.md) (XX)
        - [2.3 Abuse & Misuse](collections/papers/derivate_security/abuse_misuse.md) (XX)
    - **3. Social Ethics** (XXX)
        - [3.1 Social & Economic Impact](collections/papers/social_ethics/social_economic_impact.md) (XX)
        - [3.2 Ethical & Legal Issues](collections/papers/social_ethics/ethical_legal.md) (XX)
        - [3.3 Responsibility & Accountability](collections/papers/social_ethics/responsibility_accountability.md) (XX)
- **Datasets & Benchmarks** (XXX)
    - **1. Intrinsic Security** (XX)
        - [1.1 Robustness & Adversarial](collection/datasets/intrinsic_security/robustness.md) (XX)
        - [1.2 Hallucination](collection/datasets/intrinsic_security/hallucination.md) (XX)
        - [1.3 Interpretability](collection/datasets/intrinsic_security/interpretability.md) (XX)
    - **2. Derivative Security** (XX)
        - [2.1 Privacy](collections/benchmarks/derivate_security/privacy.md) (XX)
        - [2.2 Bias & Discrimination](collections/benchmarks/derivate_security/bias_discrimination.md) (XX)
        - [2.3 Abuse & Misuse (Deepfake)](collections/benchmarks/derivate_security/abuse_misuse.md) (XX)


## Thank you to our contributors! üôè

[![Star History Chart](https://api.star-history.com/svg?repos=YourRepo/Awesome-AI-Governance&type=Date)](https://star-history.com/#YourRepo/Awesome-AI-Governance&Date)

## Acknowledgement

This repository is maintained by the authors of the survey "Never Compromise to Vulnerabilities: A Comprehensive Survey on Al Governance" (TPAMI 2025). The taxonomy and organization are derived directly from this work. We are inspired by the open-source spirit of repositories like [Awesome-LM-SSP](https://github.com/ThuCCSLab/Awesome-LM-SSP), [LLM Security](https://llmsecurity.net/), and [Awesome LLM Security](https://github.com/corca-ai/awesome-llm-security).
