# Bias & Discrimination

- [2025/06] **[FairMT-bench: Benchmarking Fairness for Multi-turn Dialogue in Conversational LLMs](https://arxiv.org/abs/2410.19317)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800) ![Evaluation](https://img.shields.io/badge/Evaluation-87b800)

- [2025/04] **[NovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens](https://openreview.net/forum?id=uMEsKEiB7J)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800) ![ICLR](https://img.shields.io/badge/ICLR-f1b800)

- [2024/12] **[MEQA: A Benchmark for Multi-hop Event-centric Question Answering with Explanations](https://proceedings.neurips.cc/paper_files/paper/2024/hash/e560a0b22e4432003d0dba63ff8dc457-Abstract-Datasets_and_Benchmarks_Track.html)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800) ![NeurIPS](https://img.shields.io/badge/NeurIPS-f1b800)

- [2024/10] **[Benchmarking Spurious Bias in Few-shot Image Classifiers](https://eccv.ecva.net/virtual/2024/poster/1190)** ![Benchmark](https://img.shields.io/badge/Benchmark-87b800) ![Evaluation](https://img.shields.io/badge/Evaluation-87b800) ![ECCV](https://img.shields.io/badge/ECCV-f1b800)

- [2024/06] **[Identifying Fairness Issues in Automatically Generated Testing Content](https://aclanthology.org/2024.bea-1.20/)** ![Evaluation](https://img.shields.io/badge/Evaluation-87b800) ![BEA](https://img.shields.io/badge/BEA-f1b800)

- [2024/05] **[KoBBQ: Korean Bias Benchmark for Question Answering](https://aclanthology.org/2024.tacl-1.28/)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800) ![TACL](https://img.shields.io/badge/TACL-f1b800)

- [2023/10] **[FACET: Fairness in Computer Vision Evaluation Benchmark](https://openaccess.thecvf.com/content/ICCV2023/html/Gustafson_FACET_Fairness_in_Computer_Vision_Evaluation_Benchmark_ICCV_2023_paper.html)** ![Benchmark](https://img.shields.io/badge/Benchmark-87b800) ![Evaluation](https://img.shields.io/badge/Evaluation-87b800) ![ICCV](https://img.shields.io/badge/ICCV-f1b800)

- [2023/07] **[FairPrism: Evaluating Fairness-Related Harms in Text Generation](https://aclanthology.org/2023.acl-long.343/)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Evaluation](https://img.shields.io/badge/Evaluation-87b800) ![ACL](https://img.shields.io/badge/ACL-f1b800)

- [2022/12] **[MT-GenEval: A Counterfactual and Contextual Dataset for Evaluating Gender Accuracy in Machine Translation](https://aclanthology.org/2022.emnlp-main.288/)** ![Benchmark](https://img.shields.io/badge/Benchmark-87b800) ![Evaluation](https://img.shields.io/badge/Evaluation-87b800) ![EMNLP](https://img.shields.io/badge/EMNLP-f1b800)

- [2022/05] **[BBQ: A Hand-built Bias Benchmark for Question Answering](https://aclanthology.org/2022.findings-acl.165/)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800) ![Findings of ACL](https://img.shields.io/badge/Findings_of_ACL-f1b800)

- [2021/12] **[Fair SA: Sensitivity Analysis for Fairness in Face Recognition](https://proceedings.mlr.press/v171/joshi22a.html)** ![Evaluation](https://img.shields.io/badge/Evaluation-87b800) ![PMLR](https://img.shields.io/badge/PMLR-f1b800)
