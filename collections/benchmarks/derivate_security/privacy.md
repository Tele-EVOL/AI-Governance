# Privacy

- [2024/12] **[Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration](https://proceedings.neurips.cc/paper_files/paper/2024/hash/f36ad694188bb4c4bbbd61e2038e069e-Abstract-Conference.html)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Dataset](https://img.shields.io/badge/Dataset-87b800) ![Evaluation](https://img.shields.io/badge/Evaluation-87b800) ![NeurIPS](https://img.shields.io/badge/NeurIPS-f1b800)

- [2023/10] **[Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory](https://arxiv.org/abs/2310.17884)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800) ![ICLR](https://img.shields.io/badge/ICLR-f1b800)

- [2023/10] **[Beyond Memorization: Violating Privacy via Inference with Large Language Models](https://arxiv.org/abs/2310.07298)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Dataset](https://img.shields.io/badge/Dataset-87b800)

- [2023/09] **[Leveraging Large Language Models for Sequential Recommendation](https://dl.acm.org/doi/10.1145/3604915.3610639)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Dataset](https://img.shields.io/badge/Dataset-87b800) ![RecSys](https://img.shields.io/badge/RecSys-f1b800)
