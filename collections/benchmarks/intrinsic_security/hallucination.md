- [2025/01] **[A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions](https://arxiv.org/abs/2311.05232)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Survey](https://img.shields.io/badge/Survey-87b800)
- [2024/12] **[Toward adaptive reasoning in large language models with thought rollback](https://arxiv.org/abs/2412.19707)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800)
- [2024/11] **[VL-Uncertainty: Detecting Hallucination in Large Vision-Language Model via Uncertainty Estimation](https://arxiv.org/abs/2411.11919)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/10] **[Hallu-pi: Evaluating hallucination in multi-modal large language models within perturbed inputs](https://arxiv.org/abs/2408.01355)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Evaluation](https://img.shields.io/badge/Evaluation-87b800)
- [2024/10] **[Mllm can see? dynamic correction decoding for hallucination mitigation](https://arxiv.org/abs/2410.11779)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/10] **[Look twice before you answer: Memory-space visual retracing for hallucination mitigation in multimodal large language models](https://arxiv.org/abs/2410.03577)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/10] **[Fine-Tuning Large Language Models to Appropriately Abstain with Semantic Entropy](https://arxiv.org/abs/2410.17234)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800)
- [2024/07] **[Generation constraint scaling can mitigate hallucination](https://arxiv.org/abs/2407.16908)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/06] **[Detecting hallucinations in large language models using semantic entropy](https://arxiv.org/abs/2405.19648)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/06] **[Hallucination augmented contrastive learning for multimodal large language model](https://openaccess.thecvf.com/content/CVPR2024/html/Jiang_Hallucination_Augmented_Contrastive_Learning_for_Multimodal_Large_Language_Model_CVPR_2024_paper.html)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800) ![CVPR'24](https://img.shields.io/badge/CVPR'24-f1b800)
- [2024/06] **[Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation](https://arxiv.org/abs/2311.17911)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800) ![CVPR'24](https://img.shields.io/badge/CVPR'24-f1b800)
- [2024/06] **[Groundhog: Grounding large language models to holistic segmentation](https://arxiv.org/abs/2402.16846)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800) ![CVPR'24](https://img.shields.io/badge/CVPR'24-f1b800)
- [2024/06] **[Vcoder: Versatile vision encoders for multimodal large language models](https://openaccess.thecvf.com/content/CVPR2024/html/Jain_VCoder_Versatile_Vision_Encoders_for_Multimodal_Large_Language_Models_CVPR_2024_paper.html)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800) ![CVPR'24](https://img.shields.io/badge/CVPR'24-f1b800)
- [2024/05] **[Woodpecker: Hallucination correction for multimodal large language models](https://arxiv.org/abs/2310.16045)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/01] **[Vigc: Visual instruction generation and correction](https://ojs.aaai.org/index.php/AAAI/article/view/28410)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800) ![AAAI'24](https://img.shields.io/badge/AAAI'24-f1b800)
- [2023/12] **[Mitigating open-vocabulary caption hallucinations](https://arxiv.org/abs/2312.03631)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2023/11] **[Mitigating hallucination in visual language models with visual supervision](https://arxiv.org/abs/2311.16479)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2023/10] **[Ferret: Refer and ground anything anywhere at any granularity](https://arxiv.org/abs/2310.07704)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800)
