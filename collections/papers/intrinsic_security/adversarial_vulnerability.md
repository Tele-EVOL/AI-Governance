- [2025/02] **[Safety at Scale: A Comprehensive Survey of Large Model Safety](https://arxiv.org/abs/2502.05206)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Safety](https://img.shields.io/badge/Safety-87b800)
- [2024/12] **[Are we there yet? revealing the risks of utilizing large language models in scholarly peer review](https://arxiv.org/abs/2412.01708)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Evaluation](https://img.shields.io/badge/Evaluation-87b800)
- [2024/11] **[Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based Agents](https://arxiv.org/abs/2411.09523)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-964B00) ![Safety](https://img.shields.io/badge/Safety-87b800)
- [2024/11] **[TAPT: Test-Time Adversarial Prompt Tuning for Robust Inference in Vision-Language Models](https://arxiv.org/abs/2411.13136)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/10] **[AdvQDet: Detecting query-based adversarial attacks with adversarial contrastive prompt tuning](https://arxiv.org/abs/2408.01978)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800) ![MM'24](https://img.shields.io/badge/MM'24-f1b800)
- [2024/07] **[Jailbreak Attacks and Defenses Against Large Language Models: A Survey](https://arxiv.org/abs/2407.04295)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Safety](https://img.shields.io/badge/Safety-87b800)
- [2024/06] **[Mirrorcheck: Efficient adversarial defense for vision-language models](https://arxiv.org/abs/2406.09250)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/05] **[Defensive prompt patch: A robust and interpretable defense of llms against jailbreak attacks](https://arxiv.org/abs/2405.20099)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/04] **[Defending Language Models Against Image-Based Prompt Attacks via User-Provided Specifications](https://ieeexplore.ieee.org/document/10579532)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800) ![SPW'24](https://img.shields.io/badge/SPW'24-f1b800)
- [2024/02] **[LightPure: Realtime Adversarial Image Purification for Mobile Devices Using Diffusion Models](https://dl.acm.org/doi/10.1145/3636534.3690684)** ![AIGC](https://img.shields.io/badge/AIGC-a99cf4) ![Defense](https://img.shields.io/badge/Defense-87b800) ![MobiCom'24](https://img.shields.io/badge/MobiCom'24-f1b800)
- [2024/01] **[MoGU: A framework for enhancing safety of LLMs while preserving their usability](https://openreview.net/pdf?id=SrFbgIjb53)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Framework](https://img.shields.io/badge/Framework-87b800) ![Safety](https://img.shields.io/badge/Safety-87b800) ![NeurIPS'24](https://img.shields.io/badge/NeurIPS'24-f1b800)
- [2024/01] **[Mllm-protector: Ensuring mllm's safety without hurting performance](https://arxiv.org/abs/2401.02906)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/01] **[One prompt word is enough to boost adversarial robustness for pre-trained vision-language models](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_One_Prompt_Word_is_Enough_to_Boost_Adversarial_Robustness_for_CVPR_2024_paper.pdf)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800) ![CVPR'24](https://img.shields.io/badge/CVPR'24-f1b800)
- [2023/11] **[Improving alignment and robustness with circuit breakers](https://arxiv.org/abs/2406.04313)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800) ![NeurIPS'24](https://img.shields.io/badge/NeurIPS'24-f1b800)
- [2023/10] **[A mutation-based method for multi-modal jailbreaking attack detection](https://arxiv.org/abs/2310.14442)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2023/07] **[Few-shot adversarial prompt learning on vision-language models](https://arxiv.org/abs/2403.14774)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800) ![NeurIPS'24](https://img.shields.io/badge/NeurIPS'24-f1b800)
- [2023/05] **[Defense-prefix for preventing typographic attacks on clip](https://arxiv.org/abs/2304.04512)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800) ![ICCV'23](https://img.shields.io/badge/ICCV'23-f1b800)
- [2022/11] **[Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800) ![NeurIPS'22](https://img.shields.io/badge/NeurIPS'22-f1b800)
- [2022/10] **[Robust safety classifier against jailbreaking attacks: Adversarial prompt shield](https://aclanthology.org/2024.woah-1.12/)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800) ![WOAH'24](https://img.shields.io/badge/WOAH'24-f1b800)
- [2022/07] **[Towards efficient adversarial training on vision transformers](https://link.springer.com/chapter/10.1007/978-3-031-19784-0_18)** ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800) ![ECCV'22](https://img.shields.io/badge/ECCV'22-f1b800)
- [2022/05] **[Diffusion models for adversarial purification](https://arxiv.org/abs/2205.07460)** ![AIGC](https://img.shields.io/badge/AIGC-a99cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2022/05] **[When adversarial training meets vision transformers: Recipes from training to architecture](https://proceedings.neurips.cc/paper_files/paper/2022/file/760b5def8dcb1156aac454e9c0f5f406-Paper-Conference.pdf)** ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800) ![NeurIPS'22](https://img.shields.io/badge/NeurIPS'22-f1b800)
- [2022/02] **[Patch Vestiges in the Adversarial Examples Against Vision Transformer Can Be Leveraged for Adversarial Detection](https://openreview.net/pdf?id=Y3fjmc2vkKA)** ![Defense](https://img.shields.io/badge/Defense-87b800) ![AAAI-W'22](https://img.shields.io/badge/AAAI--W'22-f1b800)
- [2020/08] **[Square attack: a query-efficient black-box adversarial attack via random search](https://link.springer.com/chapter/10.1007/978-3-030-58580-8_29)** ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800) ![ECCV'20](https://img.shields.io/badge/ECCV'20-f1b800)
- [2019/06] **[Unrestricted adversarial examples via semantic manipulation](https://arxiv.org/abs/1904.06347)** ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800) ![ICCV'19](https://img.shields.io/badge/ICCV'19-f1b800)
- [2018/06] **[Boosting adversarial attacks with momentum](https://openaccess.thecvf.com/content_cvpr_2018/html/Dong_Boosting_Adversarial_Attacks_CVPR_2018_paper.html)** ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800) ![CVPR'18](https://img.shields.io/badge/CVPR'18-f1b800)
- [2018/02] **[Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples](https://proceedings.mlr.press/v80/athalye18a.html)** ![Evaluation](https://img.shields.io/badge/Evaluation-87b800) ![ICML'18](https://img.shields.io/badge/ICML'18-f1b800)
- [2017/04] **[Trojaning attack on neural networks](https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_03A-5_Liu_paper.pdf)** ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800) ![NDSS'18](https://img.shields.io/badge/NDSS'18-f1b800)
- [2017/03] **[Practical black-box attacks against machine learning](https://dl.acm.org/doi/10.1145/3052973.3053009)** ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800) ![AsiaCCS'17](https://img.shields.io/badge/AsiaCCS'17-f1b800)
- [2017/02] **[Towards evaluating the robustness of neural networks](https://ieeexplore.ieee.org/abstract/document/7958570)** ![Evaluation](https://img.shields.io/badge/Evaluation-87b800) ![S&P'17](https://img.shields.io/badge/S&P'17-f1b800)
- [2014/12] **[Explaining and harnessing adversarial examples](https://arxiv.org/abs/1412.6572)** ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800) ![ICLR'15](https://img.shields.io/badge/ICLR'15-f1b800)
- [2013/12] **[Intriguing properties of neural networks](https://arxiv.org/abs/1312.6199)** ![Evaluation](https://img.shields.io/badge/Evaluation-87b800)
