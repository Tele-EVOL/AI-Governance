# Hallucinations

- [2025/05] **[Reducing Hallucinations in Language Model-based SPARQL Query Generation Using Post-Generation Memory Retrieval](https://arxiv.org/pdf/2502.13369)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800) ![arXiv](https://img.shields.io/badge/arXiv-f1b800)

- [2025/02] **[Smoothing Out Hallucinations: Mitigating LLM Hallucination with Smoothed Knowledge Distillation](https://arxiv.org/pdf/2502.11306)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800) ![arXiv](https://img.shields.io/badge/arXiv-f1b800)

- [2025/02] **[Trust Me, I'm Wrong: LLMs Hallucinate with Certainty Despite Knowing the Answer](https://arxiv.org/pdf/2502.12964v2)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Evaluation](https://img.shields.io/badge/Evaluation-87b800) ![arXiv](https://img.shields.io/badge/arXiv-f1b800)

- [2025/01] **[Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language Models with Only Attention Sink](https://www.usenix.org/system/files/conference/usenixsecurity25/sec25cycle1-prepub-1176-wang-yining.pdf)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Attack](https://img.shields.io/badge/Attack-87b800) ![USENIX](https://img.shields.io/badge/USENIX-f1b800)

- [2025/01] **[VASparse: Towards Efficient Visual Hallucination Mitigation for Large Vision-Language Model via Visual-Aware Sparsification](https://ui.adsabs.harvard.edu/abs/2025arXiv250106553Z/abstract)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800)  

- [2025] **[A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions](https://arxiv.org/pdf/2311.05232)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Survey](https://img.shields.io/badge/Survey-87b800) ![ACM TOIS](https://img.shields.io/badge/ACM_TOIS-f1b800)  

- [2024/12] **[Toward adaptive reasoning in large language models with thought rollback](https://arxiv.org/pdf/2412.19707?)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)  

- [2024/10] **[Look twice before you answer: Memory-space visual retracing for hallucination mitigation in multimodal large language models](https://arxiv.org/pdf/2410.03577)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800)  

- [2024/10] **[Mllm can see? dynamic correction decoding for hallucination mitigation](https://arxiv.org/pdf/2410.11779)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800)  

- [2024/10] **[Fine-Tuning Large Language Models to Appropriately Abstain with Semantic Entropy](https://arxiv.org/abs/2410.17234)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)  

- [2024/10] **[Mitigating Object Hallucination via Concentric Causal Attention](https://arxiv.org/abs/2406.02069)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800) ![NeurIPS](https://img.shields.io/badge/NeurIPS-f1b800)  

- [2024/07] **[Generation Constraint Scaling Can Mitigate Hallucination](https://arxiv.org/abs/2407.16908)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)  

- [2024/04] **[Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation](https://arxiv.org/abs/2404.06809)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)  

- [2024/04] **[Skip \\n: A Simple Method to Reduce Hallucination in Large Vision-Language Models](https://openreview.net/forum?id=5csjF5sF3r)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800) ![ICLR Workshop](https://img.shields.io/badge/ICLR_Workshop-f1b800)  

- [2024/11] **[EFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2402.09801)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800)  

- [2024/02] **[Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective](https://arxiv.org/abs/2402.14545)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800)  

- [2024/04] **[Hallucination of multimodal large language models: A survey](https://arxiv.org/pdf/2404.18930?)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Survey](https://img.shields.io/badge/Survey-87b800)  

- [2024/04] **[Prescribing the Right Remedy: Mitigating Hallucinations in Large Vision-Language Models via Targeted Instruction Tuning](https://arxiv.org/pdf/2404.10332)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800)  

- [2024/04] **[Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation](http://openaccess.thecvf.com/content/CVPR2024/papers/Huang_OPERA_Alleviating_Hallucination_in_Multi-Modal_Large_Language_Models_via_Over-Trust_CVPR_2024_paper.pdf)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800) ![CVPR](https://img.shields.io/badge/CVPR-f1b800)  


- [2024/02] **[A survey on hallucination in large vision-language models](https://arxiv.org/pdf/2402.00253)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Survey](https://img.shields.io/badge/Survey-87b800)  

- [2024/02] **[Mitigating object hallucinations in large vision-language models through visual contrastive decoding](https://openaccess.thecvf.com/content/CVPR2024/papers/Leng_Mitigating_Object_Hallucinations_in_Large_Vision-Language_Models_through_Visual_Contrastive_CVPR_2024_paper.pdf)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800) ![CVPR](https://img.shields.io/badge/CVPR-f1b800)  

- [2023/09] **[Survey of hallucination in natural language generation](https://arxiv.org/pdf/2202.03629)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Survey](https://img.shields.io/badge/Survey-87b800) ![ACM CSUR](https://img.shields.io/badge/ACM_CSUR-f1b800)  

- [2023/09] **[Siren's song in the AI ocean: a survey on hallucination in large language models](https://direct.mit.edu/coli/article-pdf/doi/10.1162/coli.a.16/2535477/coli.a.16.pdf)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Survey](https://img.shields.io/badge/Survey-87b800)  


- [2023/10] **[Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models](https://arxiv.org/abs/2308.13437)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800)  

- [2023/10] **[Enhancing the Spatial Awareness Capability of Multi-Modal Large Language Model](https://arxiv.org/abs/2310.20357)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800)  

- [2023/10] **[Ferret: Refer and Ground Anything Anywhere at Any Granularity](https://arxiv.org/abs/2310.07704)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800)  

- [2023/12] **[TextSquare: Scaling up Text-Centric Visual Instruction Tuning](https://arxiv.org/abs/2404.12803)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800)  


- [2023/12] **[Mitigating Open-Vocabulary Caption Hallucinations](https://arxiv.org/abs/2312.03631)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800)  

- [2023/11] **[Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization](https://arxiv.org/abs/2311.16839)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800)  

- [2023/11] **[Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision](https://arxiv.org/abs/2311.07362)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800)  
