- [2023/11] **[Non-adversarial robustness of deep learning methods for computer vision](https://arxiv.org/pdf/2305.14986)**
- [2023/02] **[Toward certified robustness against real-world distribution shifts](https://arxiv.org/abs/2206.03669)**

- [2024/07] **[Jailbreak Attacks and Defenses Against Large Language Models: A Survey](https://arxiv.org/abs/2407.04295)**
- [2020/08] **[Square attack: a query-efficient black-box adversarial attack via random search](https://link.springer.com/chapter/10.1007/978-3-030-58580-8_29)**
- [2018/06] **[Boosting adversarial attacks with momentum](https://openaccess.thecvf.com/content_cvpr_2018/html/Dong_Boosting_Adversarial_Attacks_CVPR_2018_paper.html)**
- [2017/04] **[Trojaning attack on neural networks](https://www.google.com/search?q=https://www.ndss-symposium.org/ndss2018/proceedings/papers/ndss2018_03A-1_Liu_paper.pdf)**
- [2017/03] **[Practical black-box attacks against machine learning](https://dl.acm.org/doi/10.1145/3052973.3053009)**

- [2024/11] **[TAPT: Test-Time Adversarial Prompt Tuning for Robust Inference in Vision-Language Models](https://arxiv.org/abs/2411.13136)**
- [2024/11] **[Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based Agents](https://arxiv.org/abs/2411.09523)**
- [2024/10] **[AdvQDet: Detecting query-based adversarial attacks with adversarial contrastive prompt tuning](https://arxiv.org/abs/2408.01978)**
- [2024/06] **[Mirrorcheck: Efficient adversarial defense for vision-language models](https://arxiv.org/abs/2406.09250)**
- [2024/05] **[Defensive prompt patch: A robust and interpretable defense of llms against jailbreak attacks](https://arxiv.org/abs/2405.20099)**
- [2024/04] **[Defending Language Models Against Image-Based Prompt Attacks via User-Provided Specifications](https://ieeexplore.ieee.org/document/10579532)**
- [2024/02] **[LightPure: Realtime Adversarial Image Purification for Mobile Devices Using Diffusion Models](https://arxiv.org/abs/2409.00340)**
- [2024/01] **[Mllm-protector: Ensuring mllm's safety without hurting performance](https://arxiv.org/abs/2401.02906)**
- [2024/01] **[Are we there yet? revealing the risks of utilizing large language models in scholarly peer review](https://arxiv.org/abs/2412.01708)**
- [2024/01] **[One prompt word is enough to boost adversarial robustness for pre-trained vision-language models](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_One_Prompt_Word_is_Enough_to_Boost_Adversarial_Robustness_for_CVPR_2024_paper.pdf)**
- [2024/01] **[MoGU: A framework for enhancing safety of LLMs while preserving their usability](https://openreview.net/pdf?id=SrFbgIjb53)**
- [2023/11] **[Improving alignment and robustness with circuit breakers](https://arxiv.org/abs/2406.04313)**
- [2023/10] **[A mutation-based method for multi-modal jailbreaking attack detection](https://arxiv.org/abs/2310.14442)**
- [2023/07] **[Few-shot adversarial prompt learning on vision-language models](https://arxiv.org/abs/2403.14774)**
- [2023/05] **[Defense-prefix for preventing typographic attacks on clip](https://arxiv.org/abs/2304.04512)**
- [2022/10] **[Robust safety classifier against jailbreaking attacks: Adversarial prompt shield](https://aclanthology.org/2024.woah-1.12/)**
- [2022/07] **[Towards efficient adversarial training on vision transformers](https://link.springer.com/chapter/10.1007/978-3-031-19784-0_18)**
- [2022/05] **[When adversarial training meets vision transformers: Recipes from training to architecture](https://proceedings.neurips.cc/paper_files/paper/2022/file/760b5def8dcb1156aac454e9c0f5f406-Paper-Conference.pdf)**
- [2022/02] **[Patch Vestiges in the Adversarial Examples Against Vision Transformer Can Be Leveraged for Adversarial Detection](https://openreview.net/pdf?id=Y3fjmc2vkKA)**