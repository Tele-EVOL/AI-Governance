# Privacy and Mitigation

- [2025/08] **[Towards label-only membership inference attack against pre-trained large language models](https://www.usenix.org/system/files/conference/usenixsecurity25/sec25cycle1-prepub-1107-he.pdf)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Attack](https://img.shields.io/badge/Attack-87b800) ![USENIX Security](https://img.shields.io/badge/USENIX_Security-f1b800)

- [2025/06] **[Generating is believing: Membership inference attacks against retrieval-augmented generation](https://arxiv.org/abs/2406.19234)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Attack](https://img.shields.io/badge/Attack-87b800) ![ICASSP](https://img.shields.io/badge/ICASSP-f1b800)

- [2025/04] **[Personalized Federated Training of Diffusion Models with Privacy Guarantees](https://arxiv.org/abs/2504.00952)** ![DM](https://img.shields.io/badge/DM-c7688b) ![Federated Learning](https://img.shields.io/badge/Federated_Learning-f49558) ![Defense](https://img.shields.io/badge/Defense-87b800)

- [2025/02] **[Unveiling privacy risks in llm agent memory](https://arxiv.org/abs/2502.13172)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-a99cf4) ![Privacy](https://img.shields.io/badge/Privacy-a99cf4)

- [2025/02] **[Rtbas: Defending llm agents against prompt injection and privacy leakage](https://arxiv.org/abs/2502.08966)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-a99cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)

- [2025/02] **[Privacy-Preserving Low-Rank Adaptation against Membership Inference Attacks for Latent Diffusion Models](https://arxiv.org/html/2402.11989v3)** ![DM](https://img.shields.io/badge/DM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800) ![AAAI](https://img.shields.io/badge/AAAI-f1b800)

- [2025/02] **[PPIDM: Privacy-Preserving Inference for Diffusion Model in the Cloud](https://ieeexplore.ieee.org/document/10937222/)** ![DM](https://img.shields.io/badge/DM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800) ![Privacy](https://img.shields.io/badge/Privacy-a99cf4)

- [2024/10] **[Protecting Privacy in Multimodal Large Language Models with MLLMU-Bench](https://arxiv.org/abs/2410.22108)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Privacy](https://img.shields.io/badge/Privacy-a99cf4) ![Benchmark](https://img.shields.io/badge/Benchmark-589cf4)

- [2024/08] **[{PrivImage}: Differentially Private Synthetic Image Generation using Diffusion Models with {Semantic-Aware} Pretraining](https://arxiv.org/abs/2311.12850)** ![DM](https://img.shields.io/badge/DM-c7688b) ![Differential Privacy](https://img.shields.io/badge/Differential_Privacy-f49558) ![USENIX Security](https://img.shields.io/badge/USENIX_Security-f1b800)

- [2024/08] **[dp-promise: Differentially Private Diffusion Probabilistic Models for Image Synthesis](https://www.usenix.org/conference/usenixsecurity24/presentation/wang-haichen)** ![DM](https://img.shields.io/badge/DM-c7688b) ![Differential Privacy](https://img.shields.io/badge/Differential_Privacy-f49558) ![USENIX Security](https://img.shields.io/badge/USENIX_Security-f1b800)

- [2024/05] **[Privacy-Aware Visual Language Models](https://arxiv.org/abs/2405.17423)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Privacy](https://img.shields.io/badge/Privacy-a99cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)

- [2024/05] **[Towards Building The FederatedGPT: Federated Instruction Tuning](https://arxiv.org/abs/2305.05644)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Federated Learning](https://img.shields.io/badge/Federated_Learning-f49558) ![ICASSP](https://img.shields.io/badge/ICASSP-f1b800)

- [2024/04] **[Private Attribute Inference from Images with Vision-Language Models](https://arxiv.org/abs/2404.10618)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Attack](https://img.shields.io/badge/Attack-87b800) ![Privacy](https://img.shields.io/badge/Privacy-a99cf4)

- [2024/02] **[Prompt Stealing Attacks Against Large Language Models](https://arxiv.org/abs/2402.12959)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Attack](https://img.shields.io/badge/Attack-87b800) ![Privacy](https://img.shields.io/badge/Privacy-a99cf4)

- [2024/02] **[Do Membership Inference Attacks Work on Large Language Models?](https://arxiv.org/abs/2402.07841)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Attack](https://img.shields.io/badge/Attack-87b800) ![Privacy](https://img.shields.io/badge/Privacy-a99cf4)

- [2024/01] **[Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2305.14710&hl=zh-CN&sa=X&ei=reKdaK_9DNCt6rQP1-iY-QU&scisig=AAZF9b9O6M33XOUy3FHI8qcNT-45&oi=scholarr)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Attack](https://img.shields.io/badge/Attack-87b800) ![NAACL](https://img.shields.io/badge/NAACL-f1b800)

- [2024/01] **[Right to be forgotten in the Era of large language models: implications, challenges, and solutions](https://arxiv.org/abs/2307.03941)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Unlearning](https://img.shields.io/badge/Unlearning-87b800) ![AI Ethics](https://img.shields.io/badge/AI_Ethics-a99cf4)

- [2024/01] **[Poisonprompt: Backdoor attack on prompt-based large language models](https://arxiv.org/abs/2310.12439)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Attack](https://img.shields.io/badge/Attack-87b800) ![ICASSP](https://img.shields.io/badge/ICASSP-f1b800)

- [2023/11] **[Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration](https://arxiv.org/abs/2311.06062)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Attack](https://img.shields.io/badge/Attack-87b800) ![Fine-tuning](https://img.shields.io/badge/Fine-tuning-f49558)

- [2023/10] **[Detecting Pretraining Data from Large Language Models](https://arxiv.org/abs/2310.16789)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Privacy](https://img.shields.io/badge/Privacy-a99cf4) ![Pretraining Data](https://img.shields.io/badge/Pretraining_Data-f1b800)

- [2023/10] **[Unlearn What You Want to Forget: Efficient Unlearning for LLMs](https://arxiv.org/abs/2310.20150)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Unlearning](https://img.shields.io/badge/Unlearning-87b800) ![Defense](https://img.shields.io/badge/Defense-87b800)

- [2023/10] **[Beyond Memorization: Violating Privacy Via Inference with Large Language Models](https://arxiv.org/abs/2310.07298)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Attack](https://img.shields.io/badge/Attack-87b800) ![Privacy](https://img.shields.io/badge/Privacy-a99cf4)

- [2023/10] **[Whoâ€™s Harry Potter? Approximate Unlearning in LLMs](https://arxiv.org/abs/2310.02238)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Unlearning](https://img.shields.io/badge/Unlearning-87b800) ![Defense](https://img.shields.io/badge/Defense-87b800)

- [2023/10] **[FedBPT: Efficient Federated Black-box Prompt Tuning for Large Language Models](https://arxiv.org/abs/2310.01467)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Federated Learning](https://img.shields.io/badge/Federated_Learning-f49558) ![Prompt Tuning](https://img.shields.io/badge/Prompt_Tuning-c7688b)

- [2023/08] **[LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked](https://arxiv.org/abs/2308.07308)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800) ![Safety](https://img.shields.io/badge/Safety-a99cf4)

- [2023/05] **[Privacy-Preserving Parameter-Efficient Fine-Tuning for Large Language Model Services](https://arxiv.org/html/2305.06212v2)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Privacy](https://img.shields.io/badge/Privacy-a99cf4) ![Fine-tuning](https://img.shields.io/badge/Fine-tuning-f49558)

- [2021/10] **[Large Language Models Can Be Strong Differentially Private Learners](https://arxiv.org/abs/2110.05679)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Differential Privacy](https://img.shields.io/badge/Differential_Privacy-f49558) ![Defense](https://img.shields.io/badge/Defense-87b800)

- [2020/07] **[Privacy Risks of General-Purpose Language Models](https://ieeexplore.ieee.org/document/9152761)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Privacy](https://img.shields.io/badge/Privacy-a99cf4) ![IEEE SP](https://img.shields.io/badge/IEEE_SP-f1b800)
