# Privacy

- [2025/02] **[Towards Label-only Membership Inference Attack against Pre-trained Large Language Models](https://arxiv.org/abs/2502.18943)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Evaluation](https://img.shields.io/badge/Evaluation-87b800).

- [2024/06] **[Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models](https://aclanthology.org/2024.naacl-long.171/)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Safety](https://img.shields.io/badge/Safety-87b800) ![NAACL](https://img.shields.io/badge/NAACL-f1b800).

- [2024/05] **[CipherGPT: Secure Two-Party GPT Inference](https://eprint.iacr.org/2023/1147)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800) ![IACR](https://img.shields.io/badge/IACR-f1b800).

- [2023/11] **[PrivateLoRA for Efficient Privacy Preserving LLM](https://arxiv.org/abs/2311.14030)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800).

- [2023/09] **[DP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass](https://arxiv.org/abs/2309.06746)** ![LLM](https://img.shields.io/badge/LLM-589cf4).

- [2023/10] **[Who's Harry Potter? Approximate Unlearning in LLMs](https://arxiv.org/abs/2310.02238)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800).

- [2023/10] **[Unlearn What You Want to Forget: Efficient Unlearning for LLMs](https://arxiv.org/abs/2310.20150)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800).

- [2023/10] **[Composite Backdoor Attacks Against Large Language Models](https://arxiv.org/abs/2310.07676)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Safety](https://img.shields.io/badge/Safety-87b800).

- [2023/09] **[LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins](https://arxiv.org/abs/2309.10254)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Evaluation](https://img.shields.io/badge/Evaluation-87b800).

- [2023/08] **[A Verified Confidential Computing as a Service Framework for Privacy Preservation](https://www.usenix.org/conference/usenixsecurity23/presentation/chen-hongbo)** ![Framework](https://img.shields.io/badge/Framework-87b800) ![USENIX Security](https://img.shields.io/badge/USENIX_Security-f1b800).

- [2023/08] **[EAST: Efficient and Accurate Secure Transformer Framework for Inference](https://arxiv.org/abs/2308.09923)** ![Framework](https://img.shields.io/badge/Framework-87b800).

- [2023/07] **[PUMA: Secure Inference of LLaMA-7B in Five Minutes](https://arxiv.org/abs/2307.12533)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800).

- [2023/07] **[Detecting Personal Information in Training Corpora: an Analysis](https://aclanthology.org/2023.trustnlp-1.18/)** ![Evaluation](https://img.shields.io/badge/Evaluation-87b800) ![TrustNLP](https://img.shields.io/badge/TrustNLP-f1b800).

- [2023/07] **[Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions](https://arxiv.org/abs/2307.03941)** ![LLM](https://img.shields.io/badge/LLM-589cf4).

- [2023/05] **[LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers](https://arxiv.org/abs/2305.18396)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800).

- [2023/05] **[Membership Inference Attacks Against Language Models via Neighbourhood Comparison](https://arxiv.org/abs/2305.18462)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Evaluation](https://img.shields.io/badge/Evaluation-87b800).

- [2023/05] **[Privacy-Preserving Prompt Tuning for Large Language Model Services](https://arxiv.org/abs/2305.06212)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800).

- [2022/10] **[EW-Tune: A Framework for Privately Fine-tuning Large Language Models with Differential Privacy](https://arxiv.org/abs/2210.15042)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Framework](https://img.shields.io/badge/Framework-87b800).

- [2022/07] **[Deduplicating Training Data Mitigates Privacy Risks in Language Models](https://proceedings.mlr.press/v162/kandpal22a/kandpal22a.pdf)** ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800) ![ICML](https://img.shields.io/badge/ICML-f1b800).

- [2022/05] **[Adaptive Differential Privacy for Language Model Training](https://aclanthology.org/2022.fl4nlp-1.3/)** ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800) ![FL4NLP](https://img.shields.io/badge/FL4NLP-f1b800).

- [2022/04] **[Just Fine-Tune Twice: Selective Differential Privacy for Large Language Models](https://arxiv.org/pdf/2204.07667)** ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800).

- [2022/03] **[Quantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks](https://arxiv.org/abs/2203.03929)** ![Evaluation](https://img.shields.io/badge/Evaluation-87b800).

- [2021/11] **[Learning and Evaluating a Differentially Private Pre-trained Language Model](https://aclanthology.org/2021.findings-emnlp.102.pdf)** ![Technical_Solution](https://img.shields.io/badge/Technical_Solution-87b800) ![Findings of EMNLP](https://img.shields.io/badge/Findings_of_EMNLP-f1b800).

- [2021/08] **[Extracting Training Data from Large Language Models](https://www.usenix.org/system/files/sec21-carlini-extracting.pdf)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Safety](https://img.shields.io/badge/Safety-87b800) ![USENIX Security](https://img.shields.io/badge/USENIX_Security-f1b800).

- [2021/06] **[Data-Free Model Extraction](https://openaccess.thecvf.com/content/CVPR2021/html/Truong_Data-Free_Model_Extraction_CVPR_2021_paper.html)** ![Evaluation](https://img.shields.io/badge/Technical_Solution-87b800) ![CVPR](https://img.shields.io/badge/CVPR-f1b800).

- [2020/05] **[Privacy Risks of General-Purpose Language Models](https://ieeexplore.ieee.org/document/9152761)** ![Safety](https://img.shields.io/badge/Safety-87b800) ![IEEE S&P](https://img.shields.io/badge/IEEE_S&P-f1b800).










